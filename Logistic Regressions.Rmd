---
title: "Logistic Regression"
author: "Trevor A. Seeger"
date: "`r Sys.Date()`"
output: html_document
---

# logistic regression
```{r}
library("readxl")
dat <- read_excel("desire.xlsx")

library(ggplot2)
ggplot(data = dat, 
       mapping = aes(x = age, y = desire)) +  
  geom_point()
```

Using linear regression model
```{r}

ggplot(data = dat, mapping = aes(x = age, y = desire))+
  geom_point()+
  geom_smooth(method=lm,se=F)
```

logistic regression line
```{r}
ggplot(data = dat, mapping = aes(x = age, y = desire))+
  geom_point()+ 
  stat_smooth(method="glm",method.args=list(family="binomial"),se=FALSE)
```

```{r}
library(ISLR) # for Default data Set
summary(Default)
mylogit <- glm(default ~ balance, data = Default, family = "binomial") #binomial tells you that DV is dichotomous
coefficients(mylogit)
confint(mylogit)


mylogit <- glm(default ~ balance, data = Default, family = "binomial")
sum.coef<-summary(mylogit)$coef
est<-exp(sum.coef[,1])
print(est)
```

Example: The desire data, showing the distribution of 24 currently married andfecund women interviewed in the Fiji Fertility Survey, according to age,education, desire for more children. the data are provided in desire.xlsx fileX1= age (year)X2= education (0=none, 1=some),Y= desire for more children (0=no more, 1=more),

Fit the Logistic Regression Model to predict the probability of desire for more children using age.βi
```{r}
library(ISLR) # for Default data Set
library(aod) # for Wald test

# mylogit <- glm(default ~ balance, data = Default, family = "binomial")# The Wald Z test
# summary(mylogit)
#The Wald chi square test for full or reduced model
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 2)  #Terms tells R which terms in the model are to be tested\
newdata = data.frame(balance=1000)
predict(mylogit, newdata, type="response")

```
```{r}

model.logit = glm(desire ~ age, data = dat, family = "binomial")
summary(model.logit)
```


for every year increase, you have in age the log odds descrease of -0.3493


Construct a 95% confidence interval for the logit model.

```{r}
exp(confint(model.logit))
```

From the default data,
a. write both the logistic regression model of Default on Income and the logit transformation of this logistic regression model.
```{r}
library(ISLR) # for Default data Set

model.logit = glm(default ~ income, data = Default, family = "binomial")
summary(model.logit)
```

```{r}
exp(coefficients(model.logit))
```



b. Interpret the logistic regression coefficient  in logistic model Y eβ1^
give the odds, not just the log odds


c. Test if The probability of default depends on Income at alpha  = 0.05
When that's the only thing in the model, yes. 


d. Find a 95% Confidence Interval for the logistic regression coefficient e^beta_i
```{r}
exp(confint(model.logit))
# confint(exp(model.logit)) # does not run because it's wrong 
```

e. Use the method of Model Fit in Logistic Regression Model to evaluate the performance of a logistic regression model


f. Predict the probability of default when Income= 60,000 dollars. Would you consider a person with $60,000 income defaults on payment?
```{r}
newdata = data.frame(income=60000)
predict(model.logit, newdata, type="response")
```




next question 

```{r}
library(ggplot2)
dat=read.csv("DISCRIM.csv", header = TRUE)
mylogit <- glm(HIRE ~EXP, data = dat, family = "binomial")#option1 using ggplot function
ggplot(dat, aes(x=EXP, y=HIRE)) + 
  geom_point() +   
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE)

#option2 using plot (base graphic)
plot(dat$EXP, dat$HIRE) 
curve(predict(mylogit, data.frame(EXP=x), type="response"), add=TRUE)
```

```{r}
model.logit <- glm(factor(HIRE) ~factor(GENDER), data = dat, family = "binomial")
summary(model.logit)


```


# Logistit regression part 2
categorical variables
```{r}
library(ISLR) # for Default data Set
mylogit <- glm(default ~ balance+income, data = Default, family = "binomial")
sum.coef<-summary(mylogit)$coef

est<-exp(sum.coef[,1])
print(est)



# Option 1 to fit logitic using a contingency table
model1<-glm(yes/(yes+no)~gender+race,weights=yes+no,family = "binomial",data=marijuana)

summary(model1)  # for Wald Z test



# Option 2 to fit logitic using a contingency table
model2<-glm(as.matrix(marijuana[,3:4])~gender+race,family = "binomial",data=marijuana)# The matrix response sould not be a data frame in R
summary(model2)  # for Wald Z test



library(aod) # for Wald test
library(ISLR) # for Default data Set
mylogit <- glm(default ~ balance+income, data = Default, family = "binomial")#The Wald chi square test for the full model 
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 2:3)  #Terms tells R which terms in the model are to be tested.
#p-value for the overall test using the likelihood-ratio test
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))


```



## likelihood ratio test
```{r}
library(ISLR) # for Default data Set
simplelogit <- glm(default ~ balance, data = Default, family = "binomial")
multiplelogit <- glm(default ~ balance+income, data = Default, family = "binomial")
anova(simplelogit,multiplelogit,test="Chisq")


library(lmtest)# for lrtest() function 
#Another option to perform the Likelihood Ratio Test
lrtest(simplelogit,multiplelogit)


```



## Model fit in multiple logistic regression

```{r}
library(ISLR) # for Default data Set
library(ROCR) # for ROC 
library(pROC) # for AUC
mymullogit <- glm(default ~ balance+income, data = Default, family = "binomial")
summary(mymullogit)

mylogit <- glm(default ~ balance, data = Default, family = "binomial")
summary(mylogit)

# ROC&AUC for default = balance
#--------ROC Curve-----------
prob1=predict(mylogit,type=c("response")) #this one comes from the ROC package
# Option 1
pred<-prediction(prob1,Default$default)
perf<-performance(pred,measure = "tpr",x.measure="fpr")
plot(perf,col=2,main="ROC CURVE ", xlab="False Positive Rate (1-Specificity)",ylab="TruePositive Rate(Sensitiity)")
abline(0,1)


#  Option 2
roc1<-roc(Default$default,prob1) #from pROC package
plot(roc1) #also provides the ROC curve





#-------AUC------------------
auc(roc1)


# ROC&AUC for default = balance+income
#--------ROC Curve-----------
prob2=predict(mymullogit,type=c("response"))
pred<-prediction(prob2,Default$default)
perf<-performance(pred,measure = "tpr",x.measure="fpr")
plot(perf,col=2,main="ROC CURVE ", xlab="False Positive Rate (1-Specificity)",ylab="TruePositive Rate (Sensitiity)")
abline(0,1)

  
```

### In class problem 
Consider the problem of collusive bidding among road construction contractors.Contractors sometimes scheme to set bid prices higher than the fair market (orcompetitive) price. Suppose an investigator has obtained information on the bid status(1 if fixed bid or 0 if competitive bid) for a sample of 31 contracts. In addition, two
variables thought to be related to bid status are also recorded for each contract:number of bidders x1 and the difference between the winning (lowest) bid and theestimated competitive bid (called the engineer’s estimate) x2, measured as apercentage of the estimate. The data are provided in ROADBIDS.csv file

```{r}
dat = read.delim("ROADBIDS.txt")
## a ------------
mylogit = glm(glm(STATUS ~ NUMBIDS + DOTEST, data = dat, family = "binomial"))
summary(mylogit)
```



a: Wald test
See above

b: From the result in a) find the Wald  statistic for the full logistic regression modelwith the p-value?

```{r}
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 2:3)  #Terms tells R which terms in the model are to be tested.
```

C: likelihood ratio with reduced model with just numbids
```{r}
mylogit.red = glm(STATUS ~ NUMBIDS, data = dat, family = "binomial")
anova(mylogit.red,mylogit,test="Chisq")

```

D: write logistic regression models and logit transformations for both models. 

E: Compare model fits 
```{r}
#--------ROC Curve-----------
prob1=predict(mylogit,type=c("response")) #this one comes from the ROC package
# Option 1
pred<-prediction(prob1,dat$STATUS)
perf<-performance(pred,measure = "tpr",x.measure="fpr")
plot(perf,col=2,main="ROC CURVE ", xlab="False Positive Rate (1-Specificity)",ylab="TruePositive Rate(Sensitiity)")
abline(0,1)



# ROC&AUC for default = balance+income
#--------ROC Curve-----------
prob2=predict(mylogit.red,type=c("response"))
pred<-prediction(prob2,dat$STATUS)
perf<-performance(pred,measure = "tpr",x.measure="fpr")
plot(perf,col=2,main="ROC CURVE ", xlab="False Positive Rate (1-Specificity)",ylab="TruePositive Rate (Sensitiity)")
abline(0,1)

```


F: Predict probability of status when there are 4 bidders and difference between winning (lowest) bit and estimated competitive bid (engineers est) was 27.3% higher







### In class problem. 

Suppose you are investigating allegations of experience, level of education and genderin the hiring practices of a particular firm. Data were collected on 60 former applicantsto be used to fit the logit model whereY= 1 if hired, 0 if notX1= Years of educationX2= Years of experienceNANANA
X3=1 if male applicant, 0 if female applicantFrom the DISCRIM.csv data



```{r}
dat = read.csv("DISCRIM.csv")
## a ------------
mylogit = glm(glm(HIRE ~ EDUC + EXP + GENDER, data = dat, family = "binomial"))
summary(mylogit)
```

a: Test if The probability of hiring depends on education, experience, and/or gender at alpha = 0.5
Should we drop Education predictor?

b: Use the log likelihood ratio test to check if Education predictor should be addedinto the model.
```{r}
mylogit.red = glm(HIRE ~ EXP + GENDER, data = dat, family = "binomial")
anova(mylogit.red,mylogit,test="Chisq")
```


c: Use the model fit methods (AIC, ROC curve with AUC) to confirm that addingEducation into the model will improve the model performance

```{r}

```

d: write both the logistic regression model of HIRE on and the logit transformation ofthis logistic regression model.

e: Interpret the logistic regression coefficient c^beta1 in logistic model

f: Predict the probability of hiring when a man who has worked for 6 years, has 4years of education. Would he be hired for this firm


## Interactions Logistic Regressions
```{r}
library(lmtest)
# mylogit <- glm(default ~ balance+income, data = Default, family = "binomial")summary(mylogit)#Wald z tes
# 
# 
# 
# interlogit <- glm(default ~ balance+income+balance*income, data = Default, family = "binomial")
# summary(interlogit)
# anova(mylogit,interlogit,test="Chisq")

```

problems

```{r}
logit.int = glm(HIRE ~ EDUC +  EXP + GENDER + 
                EDUC * EXP + EDUC * GENDER + EXP * GENDER, 
                data = dat, family = "binomial")

summary(logit.int)
```



# Experimental design
## ANOVA

```{r}
#bloodpressure=read.csv("C:\\Users\\thuntida.ngamkham\\Desktop\\dataset603\\bloodpressure.csv", header=TRUE)
bloodpressure=read.csv("bloodpressure.csv")
str(bloodpressure) #Read your data set and double check that dependent and independent variables are correctly read by R


CRD<-aov(bloodpressure~treatment, data=bloodpressure) #Perform ANOVA for CRDsummary(CRD)

boxplot(bloodpressure~treatment, data=bloodpressure, main="Boxplot diagram for the different Levels") #a visual comparison of the data obtained at the different levels



## t.test ------

t.test(bloodpressure~treatment, data = bloodpressure,var.equal = T)


## ANOVA with more than two levels
MVPC=read.csv("MVPC.csv", header=TRUE)
str(MVPC)#Read your data set and double check that dependent and indepent variables are correctly read by R

CRD<-aov(Score~Treatment, data=MVPC) #Perform ANOVA for CRDboxplot(Score~Treatment, data=MVPC, main="Boxplot diagram for the different Levels") #a visual comparison of the data obtained at the different levels

summary(CRD)

```


### inclass

```{r}
dat = read.csv("lifetime.csv")

CRD<-aov(hrs~device, data=dat) #Perform ANOVA for CRDboxplot(Score~Treatment, data=MVPC, main="Boxplot diagram for the different Levels") #a visual comparison of the data obtained at the different levels

summary(CRD)
```

## estimation of model paramters

```{r}
ybar<-mean(dat$hrs[dat$device == "device6"])
tcrit<-qt(0.025,CRD$df.residual, lower.tail = F)
MSE<-sum((CRD$residuals)^2/CRD$df.residual)  #CRD$df.residual=24-8=16
r<-length(dat$hrs[dat$device == "device6"])#construct a 95% CI
LowerCI<-ybar-tcrit*sqrt(MSE/r)
UpperCI<-ybar+tcrit*sqrt(MSE/r)
CI<-cbind(LowerCI,UpperCI)
print(CI)

```



## multiple comparison testing

```{r}
# unadjusted paired t test ========
brokerstudy=read.csv("~/OneDrive - University of Calgary/MyCoursesThierry/DATA603/data/dataset603/brokerstudy.csv", header=TRUE)str(brokerstudy)

CRD<-aov(price~broker, data=brokerstudy) #Perform ANOVA for CRDsummary(CRD)


# Holme ===========
pairwise.t.test(brokerstudy$price,brokerstudy$broker, p.adj = "none")

# bonferonni ============= 
pairwise.t.test(brokerstudy$price,brokerstudy$broker, p.adj = "bonferroni")
#pvalue (brok 1 vs brok 2)=0.03863* 10 number of comparisons=0.38630.


## Fishers least signficant difference -----
#example for constructing the difference in two means for broker1&3
ybar1<-mean(brokerstudy$price[brokerstudy$broker == "broker1"])
ybar3<-mean(brokerstudy$price[brokerstudy$broker == "broker3"])
tvalue<-qt(0.025,CRD$df.residual, lower.tail = F)
MSE<-sum((CRD$residuals)^2/CRD$df.residual)
r<-length(brokerstudy$price[brokerstudy$broker == "broker1"])
LSD<-tvalue*sqrt((2*MSE)/r)LowerCI<-(ybar1-ybar3)-LSD#construct a 95% Lower CI
UpperCI<-(ybar1-ybar3)+LSD#construct a 95% Lower CICI<-cbind(LowerCI,UpperCI)print(CI)

# Or using 
library(agricolae)
LS=LSD.test(CRD,trt="broker")

# Tukey's HSD ==================
TukeyHSD(CRD, conf.level = 0.95)
par(mar = c(4, 7, 2, 1))
plot(TukeyHSD(CRD, conf.level = 0.95),las=1, col = "red")



# Neuwman Keuls =======
library(agricolae)# SNK.test() is avalible in the agricolae package for Newman-Keuls (SNK)
brokerstudy=read.csv("brokerstudy.csv", header=TRUE)
CRD<-aov(price~broker, data=brokerstudy) #Perform ANOVA for CRD
print(SNK.test(CRD,"broker",group=TRUE))#SNK.test() function can be used for Newman-Keuls test in R,

# Scheffe ==============
CRD<-aov(price~broker, data=brokerstudy) #Perform ANOVA for CRD
scheffe.test(CRD,"broker", group=TRUE,console=TRUE)

```
#### in class 
```{r}
dat = read.csv("MVPC.csv")
CRD<-aov(Score~Treatment, data=MVPC)

# Unadjusted ===============
pairwise.t.test(dat$Score,dat$Treatment, p.adj = "none")


# Holme ===========
pairwise.t.test(dat$Score,dat$Treatment, p.adj = "holm")


# bonferonni ============= 
pairwise.t.test(dat$Score,dat$Treatment, p.adj = "bonferroni")


## Fishers least signficant difference -----
#example for constructing the difference in two means for broker1&3
ybar1<-mean(dat$Score[dat$Treatment == levels(dat$Treatment)[1]])
ybar3<-mean(dat$Score[dat$Treatment == levels(dat$Treatment)[2]])
tvalue<-qt(0.025,CRD$df.residual, lower.tail = F)
MSE<-sum((CRD$residuals)^2/CRD$df.residual)
r<-length(dat$Score[dat$Treatment == levels(dat$Treatment)[1]])
LSD<-tvalue*sqrt((2*MSE)/r)
LowerCI<-(ybar1-ybar3)-LSD #construct a 95% Lower CI
UpperCI<-(ybar1-ybar3)+LSD #construct a 95% Lower CICI<-cbind(LowerCI,UpperCI)print(CI)

# Or using 
library(agricolae)
(LS=LSD.test(CRD,trt="Treatment"))

# Tukey's HSD ==================
TukeyHSD(CRD, conf.level = 0.95)
par(mar = c(4, 12, 2, 1))
plot(TukeyHSD(CRD, conf.level = 0.95),las=1, col = "red")



# Neuwman Keuls =======
library(agricolae)# SNK.test() is avalible in the agricolae package for Newman-Keuls (SNK)
CRD<-aov(Score~Treatment, data=dat) #Perform ANOVA for CRD
print(SNK.test(CRD,"Treatment",group=TRUE))#SNK.test() function can be used for Newman-Keuls test in R,

# Scheffe ==============
CRD<-aov(Score~Treatment, data=dat) #Perform ANOVA for CRD
scheffe.test(CRD,"Treatment", group=TRUE,console=TRUE)

require(ggplot2)
ggplot(data = dat, aes(x = Treatment, y = Score)) + 
  geom_boxplot()
```

## Model Adequacy (Normality Assumption, Constant Variance Assumption, Independent error term Assumption)

```{r}
library(lmtest)
par(mfrow=c(2,2))
plot(CRD)

# Homogeneity of Variances, Homoscedasticity
bartlett.test(price~broker, data=brokerstudy)

# Normality assumption ======
shapiro.test(residuals(CRD))

# Independent Error Term Assumption
bptest(CRD)

```

### Kruskal Test

```{r}
# use when normality assumption is not met
kruskal.test(price~broker, data=brokerstudy)

# comparisons for non-normally dsitributed residulas data
library(FSA)
DT = dunnTest(price~broker,data=brokerstudy,method="none")

```


#### in class
```{r}
dat = read.csv("lifetime.csv")

CRD<-aov(hrs~device, data=dat) #Perform ANOVA for CRD
summary(CRD)

par(mfrow=c(2,2))
plot(CRD)

# Homogeneity of Variances, Homoscedasticity
bartlett.test(hrs~device, data=dat)

# Normality assumption ======
shapiro.test(residuals(CRD))

# Independent Error Term Assumption
bptest(CRD)

kruskal.test(hrs~device, data=dat)

library(FSA)
DT = dunnTest(hrs~device, data=dat,method="none")
DT
```

