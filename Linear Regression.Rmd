---
title: "Data603 Class Exercises"
author: "Trevor Seeger -- 00503103"
date: "22/10/2019"
output: html_document
---

```{r setup, include=FALSE}

#First clear environment so that no old objects are used in this analysis
rm(list =ls())

### Run all code in this script to download the desired R packages 
## from either CRAN or GitHub
## Load Packages - for some computers using cloud storage, permission may be  
## denied if loading packages using code. Create a separate local file for package downloads.
## Then use menu option to point to this location for package installation. 
## This list is retained for documentation of packages used 
## (or possibly used - for review)


## IF you are using xlsx package, or saving larger files with XLConnect, be sure to add in these paramters
# options(java.parameters = "-Xmx1000m")
# options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx16g")) # Crucial to provide more memory to the java API to save larger excel files. 
## must be run before writexl packages are loaded
#gc()


# first download and use this package to conveniently install other packages
if (!require('pacman')) install.packages('pacman'); library(pacman) 


p_load(
  # Data manipulation
  "dplyr", "tidyverse",  "purrr", "purrrlyr",
  "splitstackshape", #"magrittr", "timetk", 
  # "data.table","zoo", "fillr", 

 
  
  # Statistics Packages
  "psych", "Hmisc",

  # Figures and Plots
  "ggplot2",  "ggraph",  

  #Knitting to HTML and Prettier Tables
  "knitr", #"kableExtra", 
  # "table1",
  
  
  #Knitting to word with prettier tables 
  # "flextable", # lets us knit to word with nice tables
  
  # Date Handling
  "lubridate",
  
  # Handling Excel Files (Not including .csv)
  # "rJava",
  #"readxl", "writexl", # these ones don't get along with the other ones very nicely
  # "xlsx", "xlsxjars", # These ones are needed if the files have passwords
  # "XLConnect", #"XLConnectJars",
  
  # Natural Language Processing Packages
  # "tm", "udpipe", "SnowballC", "hunspell",
  
  #LaTeX
  "tinytex"#, 
  #"xcolor", 
  
  
  )


 # tinytex::install_tinytex()

#To unload packages
#detach("package:markdown", unload = TRUE)



knitr::opts_chunk$set(echo = TRUE)





```


```{r}
# code.path = getwd()
source("./Linear Regression Functions.R")
source("./Visualize_Plots.R")
```

# Multiple Linear Model

```{r}
set.seed(232354)
# dat = read.csv('condominium.csv')
hundredths <- seq(from=0, to=1, by=.01)
nvalues = 5000

dat = data.frame("y" = round( runif(nvalues, -0, 20000), 2 ), #c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),
                 'x1' = round( runif(nvalues, -300, 800), 2 ), #c(69, 118.5, 116.5, 125, 129.9, 135, 139.9, 147.9, 160, 169.9, 134.9, 155, 169.9, 194.5, 209.9), 
                 'x2' = sample(hundredths, size=nvalues, replace=TRUE), 
                 'x3'	 = round( runif(nvalues, -0.005, 1.0049), 2 ), 
                 'x4'	 = sample(1:1000, size = nvalues, replace=T), 
                 'x5' = seq(1:nvalues), 
                 'x6' = c(1, 2, rep(c(3, 4, 5, 6), nvalues/4-2), 4, 5, 6, 7, 8, 9), 
                 'x7' = rep_len(c("R", "O", "Y", "G", "B", "I", "V"), length.out = nvalues)
) %>% 
  rbind(
    data.frame("y" = round( runif(nvalues/50, 160000, 180000), 2 ), #c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),
                 'x1' = round( runif(nvalues/50, -300, 800), 2 ), #c(69, 118.5, 116.5, 125, 129.9, 135, 139.9, 147.9, 160, 169.9, 134.9, 155, 169.9, 194.5, 209.9), 
                 'x2' = sample(hundredths, size=nvalues/50, replace=TRUE), 
                 'x3'	 = round( runif(nvalues/50, -0.005, 1.0049), 2 ), 
                 'x4'	 = sample(100:1000, size = nvalues/50, replace=T), 
                 'x5' = seq(1:(nvalues/50)), 
                 'x6' = c(1, 2, rep(c(4,11, 5, 12), nvalues/50/4-2), 20, 20, 20, 18, 20, 1), 
                 'x7' = rep_len(c("R", "O", "Y", "G", "B", "I", "V"), length.out = nvalues/50)
  )
)



dat = as.data.frame(lapply(dat, function(cc) cc[ sample(c(TRUE, NA), prob = c(0.95, 0.05), size = length(cc), replace = TRUE) ]))
```


```{r}
str(dat)
```

```{r}
source("./Visualize_Plots.R")

plot.dat(dat = dat, y = "y", x1 = "x1")
```

```{r}
source("./Descriptive Stats.R")
DescriptiveTables(df = dat, measure = "x6", round_to = 1)

# df = df[3:5000,]



```
 





```{r}
#Linear regression with all variables
# formula2test = formula("listprice ~ livingarea + floors + baths")

formula2test = formula("y~x1 + x2    + x3  + x4")
LinReg1 <- lm(formula2test, data = dat)
summary(LinReg1)
confint(LinReg1)

```


## Assumptions in LR
  1. Linearity (of *E(Y|X)*) that is, the expected value of outcome conditional on variables
  2. Normality of errors (at each value of *X*) with mean *0*
  3. Constant variance of errors
  4. Independence (outcome (between observations) and errors)
  
### 1. Linearity

  *  Scatterplots before doing LR (remember the near 0 and local fits?)
  *  Plots of residuals vs fitted values (or variables)
    - see below

```{r}
LinReg.Linearity(model = LinReg1)
```
In the Residual vs Fitted plot, we aer looking to see that the values are generally a consistent  distance from the horizontal line. i.e. there is no definitive angle or slope that can be seen. 





### 2. Normality of errors (at each value of X) with mean 0

```{r}
LinReg.NormErrors(model = LinReg1)
```



Normality test
```{r}
shap = LinReg.NormErrors.Shapiro()
```


`r LinReg.NormErrors.Shapiro()`

The above only works for n<5000 
If the sample size is large, statistical hypotheses tests have a large power (1 - probability of II type error), and hence any small difference between your distribution and the null distribution (Normal distribution) is meaningful and leads to the rejection of the null hypothesis.




### 3. Constant variance
```{r}
bp = LinReg.Variance()
```


If BP is greater than 0.05, the variance of the error term is constant. (Homoskedasticity). If the
error terms do not have constant variance, they are said to be heteroskedastic. [Tidbit from
Wikipedia: The term means “differing variance” and comes from the Greek “hetero” ('different')
and “skedasis” ('dispersion').]


### 4. Independence

    Could check for auto correlation (based on another variable or time)
    This one is one of the hardest to check for
    Need to start with a good sampling/design to get independent outcomes.


```{r}
LinReg.Indep()
```



#### A. Multicollinearity

    Variance inflation factor (VIF)
    If VIF >5 flag
```{r}
LinReg.Indep.Multico(model = LinReg1)
```


*Note*, the above will only remove one at a time, if you intend to remove more than one variable for multicollinearity, you should remove multiples manually. 



#### B. Influential points

    Cook’s distance (Di)
    If Di > 1 (or If Di > 0.5) flag

```{r}

LinReg.Indep.InflPts(model = LinReg1, dat = dat, y.var = "y" )
```

If the results of the two models are similar, we can keep the data points, otherwise, we will need to run a new model with those data deleted. 





## in class problem 2
```{r}
reduced = lm(listprice ~ livingarea + floors + baths, data = dat) # dropped bedrooms

anova(LinReg1, reduced)
```

```{r}
summary(reduced)
```

```{r}
summary(lmdat)$adj.r.squared
summary(reduced)$adj.r.squared
summary(lm(listprice ~ livingarea + baths, data = dat))$adj.r.squared

```

```{r}
sigma(lmdat) # RMSE for the full model
sigma(reduced) # Rmse for the reduced mode
sigma(lm(listprice ~ livingarea + baths, data = dat))
```

```{r}
lmred.int = lm(listprice ~ livingarea + floors + baths + livingarea*floors + livingarea*baths + floors*baths, data = dat)
# can also use (livingarea + floors + baths)^2 to get all of the interactions 
#If you just put the interactions in it forces the variables in  as well
summary(lmred.int)
```


```{r}
anova(reduced, lmred.int)
```
Not significant, thereforew we can remove the interactions from the model


## 
Data on last year’s sale (Y in 100,000s dollars) in 40 sales districts are given in the sales.csv file. This file also contains 
promotional expenditures (X1: in 1,000s dollars),
the number of active accounts (X2),
the number of competing brands (X3), 
andthe district potential (X4,coded) for each of the district
Find the best fitted model (interaction term might be included) to predict sales. Which model would you choose? Explain

```{r}
dat = read.csv('sales.csv')
modelfull = lm(Y~X1+X2+X3+X4, data = dat)
(modelfull.summ = summary(datmodel))
```

```{r}
model.dropx4 = modelfull = lm(Y~X1+X2+X3, data = dat)
(model.dropx4.summ = summary(model.dropx4))

(anova(model.dropx4, modelfull))

```

```{r}
model.dropx4x1 = lm(Y~X2+X3, data = dat)
(model.dropx4x1.summ = summary(model.dropx4))

(anova(model.dropx4x1, model.dropx4))

```

```{r}
model.intxn = lm(Y~X2+X3+X2*X3, data = dat)
(model.intxn.summ = summary(model.intxn))

(anova(model.dropx4, modelfull))

```

```{r}
anova(model.intxn, model.dropx4x1)
```

## 

```{r}
dat = read.csv('Credit.csv')

model.full = lm(Balance ~ Income + Limit + Rating + Cards + Age + Education + factor(Gender) + 
                             factor(Student) + factor(Married) + factor(Ethnicity), data  = dat)

summary(model.full)
```

```{r}
model.short = lm(Balance ~ Income + Limit + Rating + Cards + Age + 
                             factor(Student), data  = dat)

summary(model.short)
anova(model.short, model.full)
```

```{r}
model.short.inter = lm(Balance ~ (Income + Limit + Rating + Cards + Age + 
                             factor(Student))^2, data  = dat)

summary(model.short.inter)
```

```{r}
model.short.inter.trim = lm(Balance ~ Income + Limit + Rating + Cards + Age + 
                             factor(Student) + Income*Rating + Income*Age + Income*factor(Student) + Limit*Rating + 
                              Limit*factor(Student), data  = dat)

summary(model.short.inter.trim)
```


```{r}
model.short.inter.trim = lm(Balance ~ Income + Limit + Rating + Cards + Age + 
                             factor(Student) + Income*Rating + Income*factor(Student) + Limit*Rating + 
                              Limit*factor(Student), data  = dat)

summary(model.short.inter.trim)
```

```{r}
anova(model.short, model.short.inter.trim)
```



## proqual
```{r}
dat = read.csv('PRODQUAL.csv')
require(ggplot2)
ggplot(data = dat, aes(x = PRESSURE, y = QUALITY)) + 
  geom_point() + 
  geom_smooth()
model = lm(QUALITY ~ PRESSURE, data = dat)
summary(model)


model = lm(QUALITY ~ PRESSURE + I(PRESSURE^2), data = dat)
summary(model)



model = lm(QUALITY ~ PRESSURE + I(PRESSURE^2)+ I(PRESSURE^3), data = dat)
summary(model)



model = lm(QUALITY ~ PRESSURE + I(PRESSURE^2)+ I(PRESSURE^3)+ I(PRESSURE^4), data = dat)
summary(model)



model = lm(QUALITY ~ PRESSURE +I(PRESSURE^2)+ I(PRESSURE^3)+ I(PRESSURE^4)+ I(PRESSURE^5), data = dat)
summary(model)


```

```{r}
# install.packages('olsrr')
library(olsrr)

dat = read.csv('Credit.csv')

model.full = lm(Balance ~ Income + Limit + Rating + Cards + Age + Education + factor(Gender) + 
                             factor(Student) + factor(Married) + factor(Ethnicity), data  = dat)

ols_step_both_p(model.full, pent = 0.1, prem = 0.3, details = TRUE)
```

### Backward selection

```{r}
library(olsrr) #need to install the package olsrr
dat=read.csv("CREDIT.csv", header = TRUE)

fullmodel<-lm(Balance ~ number + Income + Limit + Rating + Cards + 
                Age + Education + factor(Gender) + factor(Student) + factor(Married) + Ethnicity, 
              data = dat)
backmodel=ols_step_backward_p(fullmodel, prem = 0.3, details=TRUE)
```

### Forward Selection 

```{r}
library(olsrr) #need to install the package olsrr

fullmodel<-lm(Balance ~ number + Income + Limit + Rating + Cards + 
                Age + Education + factor(Gender) + factor(Student) + factor(Married) + Ethnicity, 
              data = dat)

formodel=ols_step_forward_p(fullmodel,penter = 0.1, details=TRUE)
```

## All-Possible-Regressions Selection Procedure
### Option 1
```{r}
#Option 1
library(olsrr)
# require(ggplot2)
require(seqHMM)
firstordermodel<-lm(Balance ~ number + Income + Limit + Rating + Cards + 
                Age + Education + factor(Gender) + factor(Student) + factor(Married) + Ethnicity, 
              data = dat) #Select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largestR2 value or the smallest MSE, Mallow's Cp or AIC.
ks=ols_step_best_subset(firstordermodel, details=TRUE)

par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(ks$cp,type = "o",pch=10, xlab="Number of Variables",ylab="Cp")
plot(ks$rsq,type = "o",pch=10, xlab="Number of Variables",ylab="R^2")
#plot(ks$rss, xlab="Number of Variables",ylab= "RMSE")
plot(ks$aic,type = "o",pch=10, xlab="Number of Variables",ylab="AIC")
plot(ks$adjr,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

### Option 2

```{r}
# Option 2
#install.packages("leaps")
library(leaps) #need to install the package leaps for best.subset() function
#by default, regsubsets() only report results up to the best 8-variablemodel 
best.subset<-regsubsets(Balance ~ number + Income + Limit + Rating + Cards + 
                Age + Education + factor(Gender) + factor(Student) + factor(Married) + Ethnicity, 
              data = dat, nv=8 ) #by default, regsubsets() only reports results up to the best 8-variable model
#Model selection by exhaustive search, forward or backward stepwise, or sequential replacement#The summary() command outputs the best set of variables for each model size using RMSE.
summary(best.subset) # pick which variables in the best model. 

reg.summary<-summary(best.subset)# for the output interpretation
rsquare<-c(reg.summary$rsq)
cp<-c(reg.summary$cp)
AdjustedR<-c(reg.summary$adjr2)
RMSE<-c(reg.summary$rss)
cbind(rsquare,cp,RMSE,AdjustedR)


par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(reg.summary$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(reg.summary$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(reg.summary$rss,type = "o",pch=10, xlab="Number of Variables",ylab= "RMSE")
plot(reg.summary$adjr2,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

## Multiple regression part 3, Nov 5
### model selection


```{r}
#considering high order model between Xs and Y to improve the model
# install.packages("GGally")
library(GGally) # need to install the GGally package for ggpairs function
## Loading required package: ggplot2#option 1: using function ggpairs()
dat.trim = data.frame(
  'balance' = dat$Balance, 
  'income' = dat$Income,
  'rating' = dat$Rating, 
  'student' = dat$Student, 
  'limit' = dat$Limit, 
  'cards' = dat$Cards,
  'age' = dat$Age
  
)
#ggpairs(salarydata)
#LOESS or LOWESS: LOcally WEighted Scatter-plot Smoother
ggpairs(dat.trim,lower = list(continuous = "smooth_loess", combo ="facethist", discrete = "facetbar", na = "na"))
```



```{r}
#option2: using function pairs()
pairs(~balance + income + rating + factor(student) + limit + cards + age,data=dat.trim,panel = panel.smooth)


```


The data are provided in CLERICAL.csv file count for theseactivities on each of 52 working days. Conduct a StepwiseRegression Procedure and All-Possible-Regressionsprocedure of the data using R software package

For example, in a large metropolitan department store, thenumber of hours worked (Y) per day by the clerical staff maydepend on the followingvariables:
X1 = Number of pieces of mail processed (open, sort, etc.)
X2 = Number of money orders and gift certificates sold,
X3 = Number of window payments (customer chargeaccounts) transacted ,
X4 = Number of change order transactions processed ,
X5 = Number of checks cashed ,
X6 =Number of pieces of miscellaneous mail processed on an‘’as available’’ basis , and
X7 =Number of bus tickets sold
```{r}
dat = read.csv('CLERICAL.csv')

# install.packages('olsrr')
library(olsrr)

dat = read.csv('Credit.csv')

model.full = lm(Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7
                , data  = dat)

(bestmodel = ols_step_both_p(model.full, pent = 0.1, prem = 0.3, details = TRUE)) #stepwise regression
```

```{r}
#Option 1
library(olsrr)
# require(ggplot2)
require(seqHMM)
# firstordermodel<-lm(Balance ~ number + Income + Limit + Rating + Cards + 
#                 Age + Education + factor(Gender) + factor(Student) + factor(Married) + Ethnicity, 
#               data = dat) #Select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largestR2 value or the smallest MSE, Mallow's Cp or AIC.
ks=ols_step_best_subset(model.full, details=TRUE)

par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(ks$cp,type = "o",pch=10, xlab="Number of Variables",ylab="Cp")
plot(ks$rsq,type = "o",pch=10, xlab="Number of Variables",ylab="R^2")
#plot(ks$rss, xlab="Number of Variables",ylab= "RMSE")
plot(ks$aic,type = "o",pch=10, xlab="Number of Variables",ylab="AIC")
plot(ks$adjr,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

# linear regression part 4
## regression diagnostics
### linearity assumption

```{r}
dat = read.csv("CLERICAL.csv")
#logmodel<-lm(sale~log(tv)+radio+tv*radio, data=Advertising)

model<-lm(Y~~X2 + X4 + X5, data=dat)
summary(model)

morepower<-lm(Y~X2 + I(X2^2) + X4 + X5, data = dat)



```

## Test homoscedasticity

```{r}
#residuals plot
ggplot(cubic, aes(x=.fitted, y=.resid)) +  
  geom_point() +  
  geom_hline(yintercept = 0) +  
  geom_smooth() +  
  ggtitle("Residual plot: Residual vs Fitted values")


#a scale location plot
ggplot(cubic, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +  
  geom_point() +  
  geom_hline(yintercept = 0) +  
  geom_smooth()+   
  ggtitle("Scale-Location plot : Standardized Residual vs Fitted values")


#optional graphs for residual plots and a scale location plot
plot(cubic, which=1) #residuals plot
plot(cubic, which=3) #a scale location plot
```

```{r}
# A more formal, mathematical way of detecting heteroscedasticity is what isknown as the Breusch-Pagan test. It involves using a variance function andusing a  to test
library(lmtest)
cubic<-lm(Y~X2 + I(X2^2) + I(X2^3), data=dat)
bptest(cubic) #the Breush-pagan test for homoscedasticity


```

## Normality assumption 
```{r}
library(ggplot2)



#option 1 (histogram)
qplot(residuals(morepower),
      geom="histogram",
      # binwidth = 0.1,
      main = "Histogram of residuals",
      xlab = "residuals", color="red",
      fill=I("blue"))

#option 2 (histogram)
ggplot(data=dat, aes(residuals(morepower))) +
  geom_histogram(col="red", fill="blue")+#, breaks = seq(-1,1,by=0.1)) +
  labs(title="Histogram for residuals") +
  labs(x="residuals", y="Count")

#normal QQ plot
ggplot(data = dat, aes(sample=morepower$residuals)) +
  stat_qq() +
  stat_qq_line()

#optional histogram
par(mfrow=c(1,2))
hist(residuals(morepower))
plot(morepower, which=2) #a Normal plot

#Testing for Normality
shapiro.test(residuals(morepower)) #p>0.05 will be normal
```

```{r}
#Calculate VIF for multicollinearity model
dat = read.csv("CREDIT.csv")
firstordermodel = lm(Balance ~ Income + Age + Cards + factor(Student) + Limit, data = dat)

#option 1 
# X<-cbind(workhours$X2,workhours$X4,workhours$X5)
# imcdiag(X,workhours$Y, method="VIF")

## OR 

#option 2
library(car)
vif(firstordermodel)
#vif(improvemodel)

library(mctest) #for VIF
# workhours=read.csv("~/OneDrive - University of Calgary/MyCoursesThierry/DATA603/data/dataset603/CLERICAL.csv",
#                    header = TRUE) 
# #improvemodel<-lm(Y~X2+I(X2^2)+X4+X5,data=workhours)
# firstordermodel<-lm(Y~X2+X4+X5,data=workhours)
newfirstordermodel = lm(Balance ~ Income + Age + Cards + factor(Student), data = dat)
pairs(~Balance + Income + Age + Cards + factor(Student),data=dat)
vif(newfirstordermodel)
```



## Week 4 
From the clerical staff work hours, using residual plots to conduct a residualanalysis of the data. Check any potential outliers.
```{r}
dat = read.csv("CLERICAL.csv")
#logmodel<-lm(sale~log(tv)+radio+tv*radio, data=Advertising)

model<-lm(Y~X2 + X4 + X5, data=dat)
summary(model)

morepower<-lm(Y~X2 + I(X2^2) + X4 + X5, data = dat)


# Residual vs leverage plot
plot(morepower,which=5)

# Cook's distance
dat[cooks.distance(morepower)>0.5,] #have Cook statistics larger than 0.5
plot(morepower,pch=18,col="red",which=c(4))



# Leverage points
lev=hatvalues(morepower)
p = length(coef(morepower))
n = nrow(dat)
outlier = lev[lev>(2*p/n)] # can also use (3p/n) at tester's discretion
print(outlier)


plot(rownames(dat),lev, 
     main = "Leverage in Advertising Dataset", 
     xlab="observation",
     ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)
```

Check assumptions for the model to predict executive salary (Y)
```{r}
morepower<-lm(Y~ X1 + I(X1^2) +X2 + X3 + X4 + X5 + X3*X4, data = dat)
#residuals plot
ggplot(morepower, aes(x=.fitted, y=.resid)) +  
  geom_point() +  
  geom_hline(yintercept = 0) +  
  geom_smooth() +  
  ggtitle("Residual plot: Residual vs Fitted values")


#a scale location plot
ggplot(morepower, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +  
  geom_point() +  
  geom_hline(yintercept = 0) +  
  geom_smooth()+   
  ggtitle("Scale-Location plot : Standardized Residual vs Fitted values")

#Homoscedasticity
# A more formal, mathematical way of detecting heteroscedasticity is what isknown as the Breusch-Pagan test. It involves using a variance function andusing a  to test
library(lmtest)
bptest(morepower) #the Breush-pagan test for homoscedasticity



library(ggplot2)
#option 1 (histogram)
qplot(residuals(morepower),
      geom="histogram",
      # binwidth = 0.1,
      main = "Histogram of residuals",
      xlab = "residuals", color="red",
      fill=I("blue"))

#option 2 (histogram)
ggplot(data=dat, aes(residuals(morepower))) +
  geom_histogram(col="red", fill="blue")+#, breaks = seq(-1,1,by=0.1)) +
  labs(title="Histogram for residuals") +
  labs(x="residuals", y="Count")

#normal QQ plot
ggplot(data = dat, aes(sample=morepower$residuals)) +
  stat_qq() +
  stat_qq_line()

#optional histogram
par(mfrow=c(1,2))
hist(residuals(morepower))
plot(morepower, which=2) #a Normal plot

#Testing for Normality
shapiro.test(residuals(morepower)) #p>0.05 will be normal


```



